{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9830623",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First we import all the libraries we need for data analysis and machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dfc9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries for data analysis and machine learning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1c8b4",
   "metadata": {},
   "source": [
    "## 2. Data Loading Class\n",
    "\n",
    "This class helps us load the CSV file and look at basic information about our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24089072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully! Shape: (1827, 9)\n",
      "Basic Information about the Dataset:\n",
      "Number of rows: 1827\n",
      "Number of columns: 9\n",
      "Column names:\n",
      "['Date', 'Close (BTC)', 'Volume (BTC)', 'Close (ETH)', 'Volume (ETH)', 'Close (USDT)', 'Volume (USDT)', 'Close (BNB)', 'Volume (BNB)']\n",
      "Data types:\n",
      "Date              object\n",
      "Close (BTC)      float64\n",
      "Volume (BTC)       int64\n",
      "Close (ETH)      float64\n",
      "Volume (ETH)       int64\n",
      "Close (USDT)     float64\n",
      "Volume (USDT)      int64\n",
      "Close (BNB)      float64\n",
      "Volume (BNB)       int64\n",
      "dtype: object\n",
      "First 5 rows:\n",
      "                        Date  Close (BTC)  Volume (BTC)  Close (ETH)  \\\n",
      "0  2017-11-13 00:00:00+00:00      6559.49    6263249920       316.72   \n",
      "1  2017-11-14 00:00:00+00:00      6635.75    3197110016       337.63   \n",
      "2  2017-11-15 00:00:00+00:00      7315.54    4200880128       333.36   \n",
      "3  2017-11-16 00:00:00+00:00      7871.69    5123809792       330.92   \n",
      "4  2017-11-17 00:00:00+00:00      7708.99    4651670016       332.39   \n",
      "\n",
      "   Volume (ETH)  Close (USDT)  Volume (USDT)  Close (BNB)  Volume (BNB)  \n",
      "0    1041889984          1.01      767884032         1.69      12238800  \n",
      "1    1069680000          1.01      429857984         1.59       7829600  \n",
      "2     722665984          1.00      449671008         1.53       7615500  \n",
      "3     797254016          1.00      650278976         1.58       8928640  \n",
      "4     621732992          1.00      639398016         1.51       8508840  \n"
     ]
    }
   ],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.data = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        try:\n",
    "            self.data = pd.read_csv(self.file_path)\n",
    "            print(f\"Data loaded successfully! Shape: {self.data.shape}\")\n",
    "            return self.data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def basic_info(self):\n",
    "        if self.data is not None:\n",
    "            print(\"Basic Information about the Dataset:\")\n",
    "            print(f\"Number of rows: {self.data.shape[0]}\")\n",
    "            print(f\"Number of columns: {self.data.shape[1]}\")\n",
    "            print(\"Column names:\")\n",
    "            print(self.data.columns.tolist())\n",
    "            print(\"Data types:\")\n",
    "            print(self.data.dtypes)\n",
    "            print(\"First 5 rows:\")\n",
    "            print(self.data.head())\n",
    "        else:\n",
    "            print(\"No data loaded yet. Please load data first.\")\n",
    "\n",
    "crypto_loader = DataLoader(r'crypto_data_updated_29_november.csv')\n",
    "crypto_data = crypto_loader.load_data()\n",
    "crypto_loader.basic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a076f",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning Class\n",
    "\n",
    "This class helps us clean our data by removing missing values, duplicates, and fixing any problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c25ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Information:\n",
      "Empty DataFrame\n",
      "Columns: [Missing Count, Missing Percentage]\n",
      "Index: []\n",
      "Great! No missing values found in the data.\n",
      "Dropped rows with missing values. New shape: (1827, 9)\n",
      "Removed 0 duplicate rows.\n",
      "Data cleaning completed!\n",
      "Original shape: (1827, 9)\n",
      "Final shape: (1827, 9)\n"
     ]
    }
   ],
   "source": [
    "class DataCleaner:\n",
    "    \"\"\"\n",
    "    This class cleans our data by handling missing values, removing duplicates,\n",
    "    and dealing with outliers that might cause problems in our analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data.copy()\n",
    "        self.original_shape = data.shape\n",
    "    \n",
    "    def check_missing_values(self):\n",
    "        missing_values = self.data.isnull().sum()\n",
    "        missing_percent = (missing_values / len(self.data)) * 100\n",
    "        \n",
    "        missing_info = pd.DataFrame({\n",
    "            'Missing Count': missing_values,\n",
    "            'Missing Percentage': missing_percent\n",
    "        })\n",
    "        \n",
    "        print(\"Missing Values Information:\")\n",
    "        print(missing_info[missing_info['Missing Count'] > 0])\n",
    "        \n",
    "        if missing_values.sum() == 0:\n",
    "            print(\"Great! No missing values found in the data.\")\n",
    "        \n",
    "        return missing_info\n",
    "    \n",
    "    def handle_missing_values(self, method='drop'):\n",
    "        if method == 'drop':\n",
    "            self.data = self.data.dropna()\n",
    "            print(f\"Dropped rows with missing values. New shape: {self.data.shape}\")\n",
    "        elif method == 'fill':\n",
    "            numeric_columns = self.data.select_dtypes(include=[np.number]).columns\n",
    "            self.data[numeric_columns] = self.data[numeric_columns].fillna(self.data[numeric_columns].mean())\n",
    "            print(\"Filled missing values with column averages.\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        before_count = len(self.data)\n",
    "        self.data = self.data.drop_duplicates()\n",
    "        after_count = len(self.data)\n",
    "        removed_count = before_count - after_count\n",
    "        \n",
    "        print(f\"Removed {removed_count} duplicate rows.\")\n",
    "        return self.data\n",
    "    \n",
    "    def handle_outliers(self, columns=None, method='iqr'):\n",
    "        if columns is None:\n",
    "            columns = self.data.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        before_count = len(self.data)\n",
    "        \n",
    "        for column in columns:\n",
    "            if column in self.data.columns:\n",
    "                Q1 = self.data[column].quantile(0.25)\n",
    "                Q3 = self.data[column].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                # Removing outliers\n",
    "                self.data = self.data[\n",
    "                    (self.data[column] >= lower_bound) & \n",
    "                    (self.data[column] <= upper_bound)\n",
    "                ]\n",
    "        \n",
    "        after_count = len(self.data)\n",
    "        removed_count = before_count - after_count\n",
    "        print(f\"Removed {removed_count} outlier rows using IQR method.\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def get_clean_data(self):\n",
    "        print(f\"Data cleaning completed!\")\n",
    "        print(f\"Original shape: {self.original_shape}\")\n",
    "        print(f\"Final shape: {self.data.shape}\")\n",
    "        return self.data\n",
    "\n",
    "cleaner = DataCleaner(crypto_data)\n",
    "cleaner.check_missing_values()\n",
    "cleaner.handle_missing_values(method='drop')\n",
    "cleaner.remove_duplicates()\n",
    "clean_crypto_data = cleaner.get_clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710fbf99",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Class\n",
    "\n",
    "This class prepares our data for machine learning by converting dates and scaling numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a5062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed date column and created new date features.\n",
      "Created new features from existing data.\n",
      "\n",
      "Processed data shape: (1827, 16)\n",
      "\n",
      "New columns created:\n",
      "['Date', 'Close (BTC)', 'Volume (BTC)', 'Close (ETH)', 'Volume (ETH)', 'Close (USDT)', 'Volume (USDT)', 'Close (BNB)', 'Volume (BNB)', 'Year', 'Month', 'Day', 'DayOfWeek', 'BTC_ETH_Ratio', 'BTC_Volume_Price_Ratio', 'ETH_Volume_Price_Ratio']\n"
     ]
    }
   ],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    This class prepares our data for machine learning by converting dates,\n",
    "    creating new features, and scaling numerical values.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data.copy()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaled_columns = []\n",
    "    \n",
    "    def process_date_column(self, date_column='Date'):\n",
    "        if date_column in self.data.columns:\n",
    "            self.data[date_column] = pd.to_datetime(self.data[date_column])\n",
    "            \n",
    "            # Extract date features\n",
    "            self.data['Year'] = self.data[date_column].dt.year\n",
    "            self.data['Month'] = self.data[date_column].dt.month\n",
    "            self.data['Day'] = self.data[date_column].dt.day\n",
    "            self.data['DayOfWeek'] = self.data[date_column].dt.dayofweek\n",
    "            \n",
    "            print(f\"Processed date column and created new date features.\")\n",
    "        else:\n",
    "            print(f\"Column {date_column} not found in data.\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def create_features(self):\n",
    "        # Create price ratios between different cryptocurrencies\n",
    "        if 'Close (BTC)' in self.data.columns and 'Close (ETH)' in self.data.columns:\n",
    "            self.data['BTC_ETH_Ratio'] = self.data['Close (BTC)'] / self.data['Close (ETH)']\n",
    "        \n",
    "        # Create volume to price ratios\n",
    "        if 'Volume (BTC)' in self.data.columns and 'Close (BTC)' in self.data.columns:\n",
    "            self.data['BTC_Volume_Price_Ratio'] = self.data['Volume (BTC)'] / self.data['Close (BTC)']\n",
    "        \n",
    "        if 'Volume (ETH)' in self.data.columns and 'Close (ETH)' in self.data.columns:\n",
    "            self.data['ETH_Volume_Price_Ratio'] = self.data['Volume (ETH)'] / self.data['Close (ETH)']\n",
    "        \n",
    "        print(\"Created new features from existing data.\")\n",
    "        return self.data\n",
    "    \n",
    "    def scale_numerical_features(self, columns_to_scale=None):\n",
    "        if columns_to_scale is None:\n",
    "            columns_to_scale = self.data.select_dtypes(include=[np.number]).columns\n",
    "            # Remove date-related columns from scaling\n",
    "            columns_to_scale = [col for col in columns_to_scale if col not in ['Year', 'Month', 'Day', 'DayOfWeek']]\n",
    "        \n",
    "        self.scaled_columns = columns_to_scale\n",
    "        self.data[columns_to_scale] = self.scaler.fit_transform(self.data[columns_to_scale])\n",
    "        \n",
    "        print(f\"Scaled {len(columns_to_scale)} numerical columns.\")\n",
    "        return self.data\n",
    "    \n",
    "    def prepare_for_modeling(self, target_column='Close (BTC)'):\n",
    "        # Remove non-numeric columns and the target column from features\n",
    "        feature_columns = self.data.select_dtypes(include=[np.number]).columns\n",
    "        feature_columns = [col for col in feature_columns if col != target_column]\n",
    "        \n",
    "        X = self.data[feature_columns]\n",
    "        y = self.data[target_column]\n",
    "        \n",
    "        print(f\"Prepared {len(feature_columns)} features to predict {target_column}\")\n",
    "        print(f\"Feature columns: {feature_columns}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def get_processed_data(self):\n",
    "        \"\"\"\n",
    "        Return the fully processed data.\n",
    "        \"\"\"\n",
    "        return self.data\n",
    "\n",
    "# Preprocess our crypto data\n",
    "preprocessor = DataPreprocessor(clean_crypto_data)\n",
    "processed_data = preprocessor.process_date_column()\n",
    "processed_data = preprocessor.create_features()\n",
    "processed_data = preprocessor.scale_numerical_features()\n",
    "\n",
    "# Show the processed data\n",
    "print(\"Processed data shape:\", processed_data.shape)\n",
    "print(\"New columns created:\")\n",
    "print(processed_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a482a2b8",
   "metadata": {},
   "source": [
    "## 5. Save Clean CSV Class\n",
    "\n",
    "This class saves our cleaned and processed data to a new CSV file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1b440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to clean_crypto_data.csv\n",
      "Saved 1827 rows and 16 columns\n",
      "Summary saved to data_summary.txt\n"
     ]
    }
   ],
   "source": [
    "class DataSaver:\n",
    "    \"\"\"\n",
    "    This class saves our cleaned and processed data to CSV files\n",
    "    so we can use them later without repeating all the cleaning steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def save_to_csv(self, filename, include_index=False):\n",
    "        try:\n",
    "            self.data.to_csv(filename, index=include_index)\n",
    "            print(f\"Data saved successfully to {filename}\")\n",
    "            print(f\"Saved {len(self.data)} rows and {len(self.data.columns)} columns\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "    \n",
    "    def save_summary(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(\"Data Summary Report\")\n",
    "                f.write(f\"Total rows: {len(self.data)}\")\n",
    "                f.write(f\"Total columns: {len(self.data.columns)}\")\n",
    "                f.write(\"Column names:\")\n",
    "                for col in self.data.columns:\n",
    "                    f.write(f\"- {col}\")\n",
    "                f.write(\"\\n\")\n",
    "                f.write(\"Basic statistics:\")\n",
    "                f.write(str(self.data.describe()))\n",
    "            \n",
    "            print(f\"Summary saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving summary: {e}\")\n",
    "\n",
    "# Save our processed data\n",
    "data_saver = DataSaver(processed_data)\n",
    "data_saver.save_to_csv('clean_crypto_data.csv')\n",
    "data_saver.save_summary('data_summary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6636e3e",
   "metadata": {},
   "source": [
    "## 6. Model Training Class\n",
    "\n",
    "This class trains machine learning models to predict cryptocurrency prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78951d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 14 features to predict Close (BTC)\n",
      "Feature columns: ['Volume (BTC)', 'Close (ETH)', 'Volume (ETH)', 'Close (USDT)', 'Volume (USDT)', 'Close (BNB)', 'Volume (BNB)', 'Year', 'Month', 'Day', 'DayOfWeek', 'BTC_ETH_Ratio', 'BTC_Volume_Price_Ratio', 'ETH_Volume_Price_Ratio']\n",
      "Data split completed!\n",
      "Training set: 1461 samples\n",
      "Testing set: 366 samples\n",
      "Training models...\n",
      "==============================\n",
      "Training Linear Regression...\n",
      "Linear Regression training completed!\n",
      "Training Random Forest...\n",
      "Random Forest training completed!\n",
      "\n",
      "All models trained successfully!\n",
      "Random Forest training completed!\n",
      "\n",
      "All models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Initialize the trainer so we can initialize with features and target variable.\n",
    "        X: feature variables (what we use to predict)\n",
    "        y: target variable (what we want to predict)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.models = {}\n",
    "        self.trained_models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "        # Initialize different regression models\n",
    "        self.models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        }\n",
    "    \n",
    "    def split_data(self, test_size=0.2, random_state=42):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        print(f\"Data split completed!\")\n",
    "        print(f\"Training set: {len(self.X_train)} samples\")\n",
    "        print(f\"Testing set: {len(self.X_test)} samples\")\n",
    "        \n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "    \n",
    "    def train_models(self):\n",
    "        print(\"Training models...\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"Training {model_name}...\")\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            self.trained_models[model_name] = model\n",
    "            \n",
    "            print(f\"{model_name} training completed!\")\n",
    "        \n",
    "        print(\"All models trained successfully!\")\n",
    "    \n",
    "\n",
    "    \n",
    "    def plot_predictions(self, model_name):\n",
    "        if model_name in self.trained_models:\n",
    "            model = self.trained_models[model_name]\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(self.y_test, y_pred, alpha=0.6)\n",
    "            plt.plot([self.y_test.min(), self.y_test.max()], \n",
    "                    [self.y_test.min(), self.y_test.max()], \n",
    "                    'r--', lw=2)\n",
    "            plt.xlabel('Actual Values')\n",
    "            plt.ylabel('Predicted Values')\n",
    "            plt.title(f'{model_name}: Actual vs Predicted Values')\n",
    "            \n",
    "            # Add R2 score to the plot\n",
    "            r2 = self.results[model_name]['R2 Score']\n",
    "            plt.text(0.05, 0.95, f'RÂ² = {r2:.3f}', \n",
    "                    transform=plt.gca().transAxes, \n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Model {model_name} not found or not trained yet.\")\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        if not self.results:\n",
    "            print(\"No models evaluated yet. Please run evaluate_models() first.\")\n",
    "            return None\n",
    "        \n",
    "        best_model_name = max(self.results.keys(), key=lambda x: self.results[x]['R2 Score'])\n",
    "        best_score = self.results[best_model_name]['R2 Score']\n",
    "        \n",
    "        print(f\"Best performing model: {best_model_name}\")\n",
    "        print(f\"Best R2 Score: {best_score:.4f}\")\n",
    "        \n",
    "        return best_model_name, self.trained_models[best_model_name]\n",
    "    \n",
    "    def feature_importance(self, model_name):\n",
    "        if model_name in self.trained_models:\n",
    "            model = self.trained_models[model_name]\n",
    "            \n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'Feature': self.X.columns,\n",
    "                    'Importance': model.feature_importances_\n",
    "                }).sort_values('Importance', ascending=False)\n",
    "                \n",
    "                print(f\"Feature Importance for {model_name}:\")\n",
    "                print(importance_df)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.barh(importance_df['Feature'][:10], importance_df['Importance'][:10])\n",
    "                plt.xlabel('Importance')\n",
    "                plt.title(f'Top 10 Feature Importance - {model_name}')\n",
    "                plt.gca().invert_yaxis()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                return importance_df\n",
    "            else:\n",
    "                print(f\"Model {model_name} does not provide feature importance.\")\n",
    "        else:\n",
    "            print(f\"Model {model_name} not found.\")\n",
    "\n",
    "X, y = preprocessor.prepare_for_modeling(target_column='Close (BTC)')\n",
    "model_trainer = ModelTrainer(X, y)\n",
    "X_train, X_test, y_train, y_test = model_trainer.split_data(test_size=0.2)\n",
    "model_trainer.train_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
